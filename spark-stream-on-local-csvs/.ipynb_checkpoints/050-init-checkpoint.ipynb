{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 17.0.8\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T23:01:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Logistic Regression using PySpark\n",
    "This is a code along of the famous titanic dataset, its always nice to start off with this dataset because it is an example you will find across pretty much every data analysis language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-30 13:36:49--  https://raw.githubusercontent.com/meddash-cloud/meddash-public-datasets/main/data/hack_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14268 (14K) [text/plain]\n",
      "Saving to: ‘hack_data.csv’\n",
      "\n",
      "hack_data.csv       100%[===================>]  13.93K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-08-30 13:36:49 (73.6 MB/s) - ‘hack_data.csv’ saved [14268/14268]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/meddash-cloud/meddash-public-datasets/main/data/walmart_stock.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the CSV file to Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"spark-stream-csv-minio-01\"\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "minio_key = \"minio\"\n",
    "minio_secret = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"walmart_stock.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to minio minio-service.kubeflow.svc.cluster.local:9000\n",
      "try to find bucket spark-stream-csv-minio-01\n",
      "found False\n",
      "upload file to minio...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './hack_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BUCKET_NAME))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload file to minio...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mminio_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupload_file_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/minio/api.py:983\u001b[0m, in \u001b[0;36mMinio.fput_object\u001b[0;34m(self, bucket_name, object_name, file_path, content_type, metadata, sse, progress, part_size, num_parallel_uploads, tags, retention, legal_hold)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfput_object\u001b[39m(\u001b[38;5;28mself\u001b[39m, bucket_name, object_name, file_path,\n\u001b[1;32m    935\u001b[0m                 content_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/octet-stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    936\u001b[0m                 metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    937\u001b[0m                 part_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_parallel_uploads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    938\u001b[0m                 tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, legal_hold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    939\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m    Uploads data from a file to an object in a bucket.\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m        )\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 983\u001b[0m     file_size \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_size\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_data:\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_object(\n\u001b[1;32m    986\u001b[0m             bucket_name, object_name, file_data, file_size,\n\u001b[1;32m    987\u001b[0m             content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m             tags\u001b[38;5;241m=\u001b[39mtags, retention\u001b[38;5;241m=\u001b[39mretention, legal_hold\u001b[38;5;241m=\u001b[39mlegal_hold,\n\u001b[1;32m    991\u001b[0m         )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hack_data.csv'"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "\n",
    "upload_file_name=csv_file_name\n",
    "upload_file_path=\"./{}\".format(upload_file_name)\n",
    "config = {\n",
    "    \"endpoint\": minio_url,\n",
    "    \"access_key\": \"minio\",\n",
    "    \"secret_key\": \"minio123\",\n",
    "    \"secure\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a client with the MinIO server playground, its access key\n",
    "# and secret key.\n",
    "print (\"connecting to minio {}\".format(minio_url))\n",
    "minio_client = Minio(**config)\n",
    "\n",
    "print(\"try to find bucket {}\".format(BUCKET_NAME))\n",
    "found = minio_client.bucket_exists(BUCKET_NAME)\n",
    "print(\"found\", found)\n",
    "if not found:\n",
    "    minio_client.make_bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(\"Bucket '{}' already exists\".format(BUCKET_NAME))\n",
    "\n",
    "print(\"upload file to minio...\")\n",
    "minio_client.fput_object(BUCKET_NAME, os.path.basename(upload_file_path), upload_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_file_path=\"s3a://{}\".format(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"ReadTextFilesFromS3\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4')\\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", \"http://\"+minio_url)\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
