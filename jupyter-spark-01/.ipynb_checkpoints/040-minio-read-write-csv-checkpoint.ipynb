{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91284b97-16a1-48a9-8444-32f1d51f9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 17.0.8\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T23:01:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9255c3fd-b0e4-4b2a-920d-0ad8a98ae6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#generate 4-columns data\n",
    "data = np.random.rand(1000,5)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"random_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94087c5d-b552-400b-bc8a-21b8d734d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"jupyter-spark-01\"\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "minio_key = \"minio\"\n",
    "minio_secret = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa36d41-a6b7-474b-b4ba-d0a6e23fc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to minio minio-service.kubeflow.svc.cluster.local:9000\n",
      "try to find bucket jupyter-spark-01\n",
      "found True\n",
      "Bucket 'jupyter-spark-01' already exists\n",
      "upload file to minio...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7f8e0da399d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "\n",
    "upload_file_name=\"random_data.csv\"\n",
    "upload_file_path=\"./{}\".format(upload_file_name)\n",
    "config = {\n",
    "    \"endpoint\": minio_url,\n",
    "    \"access_key\": \"minio\",\n",
    "    \"secret_key\": \"minio123\",\n",
    "    \"secure\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a client with the MinIO server playground, its access key\n",
    "# and secret key.\n",
    "print (\"connecting to minio {}\".format(minio_url))\n",
    "minio_client = Minio(**config)\n",
    "\n",
    "print(\"try to find bucket {}\".format(BUCKET_NAME))\n",
    "found = minio_client.bucket_exists(BUCKET_NAME)\n",
    "print(\"found\", found)\n",
    "if not found:\n",
    "    minio_client.make_bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(\"Bucket '{}' already exists\".format(BUCKET_NAME))\n",
    "\n",
    "print(\"upload file to minio...\")\n",
    "minio_client.fput_object(BUCKET_NAME, os.path.basename(upload_file_path), upload_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99caecf3-c211-4e06-b157-83c5674d0315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://jupyter-spark-01/random_data.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minio_file_path = 's3a://{}/{}'.format(BUCKET_NAME, upload_file_name)\n",
    "minio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b898dfa0-199f-4c51-84c7-ef20f9986e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
      "hadoop-client-api-3.3.4.jar\n",
      "hadoop-client-runtime-3.3.4.jar\n",
      "hadoop-shaded-guava-1.1.1.jar\n",
      "hadoop-yarn-server-web-proxy-3.3.4.jar\n",
      "parquet-hadoop-1.12.3.jar\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/spark/jars|grep hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0765c8e3-8c33-4487-bb91-bc87f59c970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf spark_jars\n",
    "mkdir -p spark_jars\n",
    "\n",
    "wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar -P spark_jars\n",
    "wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P spark_jars\n",
    "wget -q https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar -P spark_jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaafe06-0bfa-40e9-853f-9492bb48c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8062fbee-587c-49e4-8eb5-b6f5d5f4c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_main.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile spark_main.py\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# .config(\"spark.jars\", \"spark_jars/hadoop-aws-2.7.3.jar\") \\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.3')\\\n",
    "#.config(\"spark.executor.extraClassPath\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.6')\\\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.6 pyspark-shell'\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "\n",
    "\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"ReadTextFilesFromS3\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4')\\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", \"http://\"+minio_url)\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "jars = sc._jsc.sc().listJars()\n",
    "print(\"recognized additional jars:\",jars)\n",
    "\n",
    "\n",
    "minio_file_path = \"s3a://jupyter-spark-01/random_data.csv\"\n",
    "print(minio_file_path)\n",
    "df = spark.read.csv(minio_file_path,header=False)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e22a5ebb-5578-4bdf-b9a9-dae2c6457bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.4.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0ec3371b-0b2f-41a5-82d6-4ee42f7b9525;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 545ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0ec3371b-0b2f-41a5-82d6-4ee42f7b9525\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/15ms)\n",
      "23/08/25 17:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/25 17:56:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/25 17:56:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "recognized additional jars: Vector(spark://notebook-sample-v1alpha1-0:40839/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar, spark://notebook-sample-v1alpha1-0:40839/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark://notebook-sample-v1alpha1-0:40839/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar)\n",
      "s3a://jupyter-spark-01/random_data.csv\n",
      "23/08/25 17:56:48 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "+----+-------------------+-------------------+-------------------+--------------------+-------------------+\n",
      "| _c0|                _c1|                _c2|                _c3|                 _c4|                _c5|\n",
      "+----+-------------------+-------------------+-------------------+--------------------+-------------------+\n",
      "|null|                  0|                  1|                  2|                   3|                  4|\n",
      "|   0| 0.7824329787271265| 0.8494816143472856| 0.4566505827708065| 0.31741798231168683| 0.3624834047835521|\n",
      "|   1| 0.9306382933620317| 0.6593264239096905| 0.1924425206771977|  0.4132911476958434| 0.7896714947504235|\n",
      "|   2| 0.9996068335120459|  0.823411776138118| 0.6882138833787522|  0.6883723970506531|0.19493345756389413|\n",
      "|   3| 0.3615285434239859| 0.5861525573855801| 0.9745537876712567| 0.32991849675918994| 0.5944364135098379|\n",
      "|   4| 0.9346528031801516| 0.5058996548672203|0.18184919592362758|0.024279460749197734|0.02877363778793951|\n",
      "|   5| 0.9161839631465313|0.40497502880912895| 0.5932665402008978|  0.9100109356961307| 0.9823439328859267|\n",
      "|   6|0.30326421687422145|0.32614770588222863| 0.9182949206519934|  0.5052270749857461| 0.4514653085960143|\n",
      "|   7| 0.6974491686744532| 0.0338057875548472|  0.988839067226336|  0.4348312411345172| 0.4584746769581609|\n",
      "|   8|0.19976942138975406| 0.7054196783687416| 0.9066222126671025| 0.11084254967033536|0.16012214037389894|\n",
      "|   9|  0.993395882091335|0.21004515158894044|  0.947495512965643| 0.44659789780222414| 0.5595131772049382|\n",
      "|  10|0.35541825714820674| 0.1330595755468751| 0.2501831135090328|  0.9272461896010569| 0.1857870304265291|\n",
      "|  11| 0.8671054365755024|0.35222327458424496|  0.909330675740441|  0.5529503659272514| 0.9816477761167164|\n",
      "|  12|0.28717464746833377|0.29653872662662795|  0.396132928987207|  0.7753306885117203| 0.4235464315633175|\n",
      "|  13| 0.6856316891947365|  0.207818797175906|   0.39147284833062|  0.8580678719572878| 0.9423441683737007|\n",
      "|  14| 0.3487023911474958|  0.189912492155903| 0.8474272172863863|  0.2403105133288559|  0.753374629545791|\n",
      "|  15|0.28027898870500534|0.06945495274423386| 0.0436223144287361| 0.24525045868882067| 0.4857786992141855|\n",
      "|  16| 0.6308223950183399| 0.3817420428724557| 0.5391040399392352|  0.6518131521003008| 0.1119452425663987|\n",
      "|  17| 0.8403026904844646|0.16711988470723882| 0.6689412525841886|  0.2251272313162982|0.21305052534573377|\n",
      "|  18| 0.7747243595395202| 0.8251681157540045|0.43506042089093533| 0.06623276000080691| 0.4958498823454309|\n",
      "+----+-------------------+-------------------+-------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !python spark_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635401f-25d9-4563-b39e-3a2226c75703",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://{}\".format(minio_url))\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.access.key\", minio_key)\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.secret.key\", minio_secret )\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "df = spark.read.csv(,header=False)\n",
    "df.write.format('csv').options(delimiter=',').mode('overwrite').save('s3a://{}/{}'.format(BUCKET_NAME, \"{}.overwrite.txt\".fomrat(upload_file_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655421b-a5ff-479b-a483-c9e7fe4f5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/spark/examples/src/main/python/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a51902-8a0d-4de2-abf2-197ca65173a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a5d63-972f-43c0-ab26-716dd496604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/spark/jars|grep -i hadoop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
