{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91284b97-16a1-48a9-8444-32f1d51f9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 17.0.8\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T23:01:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002f41a8-c8b6-4b71-8c4a-7f705d65aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.11/site-packages (7.1.16)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2023.7.22)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install  minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9255c3fd-b0e4-4b2a-920d-0ad8a98ae6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#generate 4-columns data\n",
    "data = np.random.rand(1000,5)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"random_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94087c5d-b552-400b-bc8a-21b8d734d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"jupyter-spark-01\"\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "minio_key = \"minio\"\n",
    "minio_secret = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa36d41-a6b7-474b-b4ba-d0a6e23fc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to minio minio-service.kubeflow.svc.cluster.local:9000\n",
      "try to find bucket jupyter-spark-01\n",
      "found True\n",
      "Bucket 'jupyter-spark-01' already exists\n",
      "upload file to minio...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7fb87f785950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "\n",
    "upload_file_name=\"random_data.csv\"\n",
    "upload_file_path=\"./{}\".format(upload_file_name)\n",
    "config = {\n",
    "    \"endpoint\": minio_url,\n",
    "    \"access_key\": \"minio\",\n",
    "    \"secret_key\": \"minio123\",\n",
    "    \"secure\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a client with the MinIO server playground, its access key\n",
    "# and secret key.\n",
    "print (\"connecting to minio {}\".format(minio_url))\n",
    "minio_client = Minio(**config)\n",
    "\n",
    "print(\"try to find bucket {}\".format(BUCKET_NAME))\n",
    "found = minio_client.bucket_exists(BUCKET_NAME)\n",
    "print(\"found\", found)\n",
    "if not found:\n",
    "    minio_client.make_bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(\"Bucket '{}' already exists\".format(BUCKET_NAME))\n",
    "\n",
    "print(\"upload file to minio...\")\n",
    "minio_client.fput_object(BUCKET_NAME, os.path.basename(upload_file_path), upload_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99caecf3-c211-4e06-b157-83c5674d0315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://jupyter-spark-01/random_data.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minio_file_path = 's3a://{}/{}'.format(BUCKET_NAME, upload_file_name)\n",
    "minio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaafe06-0bfa-40e9-853f-9492bb48c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8062fbee-587c-49e4-8eb5-b6f5d5f4c7fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognized additional jars: Vector(spark://notebook-sample-v1alpha1-0:43529/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar, spark://notebook-sample-v1alpha1-0:43529/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark://notebook-sample-v1alpha1-0:43529/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar)\n",
      "s3a://jupyter-spark-01/random_data.csv\n",
      "+----+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "| _c0|                _c1|                 _c2|                _c3|                _c4|                _c5|\n",
      "+----+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "|null|                  0|                   1|                  2|                  3|                  4|\n",
      "|   0|0.41820985915857456| 0.09309416119920544| 0.7301592997976444|0.44571530320065755| 0.4856052119719031|\n",
      "|   1|0.21461473789052588| 0.42951363901677364| 0.5156433054351429|0.02312015441580073|0.20032365884756587|\n",
      "|   2|0.14853921690617045|  0.9337026982275769| 0.3367949865091945|0.42424893392327323| 0.8592244463676983|\n",
      "|   3| 0.9688173276276156| 0.41144736146168137|0.16384465589047048| 0.3446978863183656| 0.9508910146867072|\n",
      "|   4| 0.7014735424138371|0.017709643815431164| 0.4670605635161851| 0.8027687471014326|0.19398133208756219|\n",
      "|   5|0.02223623939492636|  0.5796187293399613| 0.4603783866268988|0.24951169987089528| 0.7937832286711819|\n",
      "|   6| 0.4180237597317308|   0.930747408346505|0.36244111863806916| 0.8416061483329974| 0.3649047138043473|\n",
      "|   7| 0.9018393867863501|  0.7744992080515535| 0.5873541486175404| 0.2973395397311883| 0.4846596256576139|\n",
      "|   8| 0.5168747476632006| 0.33149572146738715| 0.7802838617293001| 0.7210616764330744|0.32129416708629444|\n",
      "|   9| 0.6755783004186103|  0.6016399829090802|0.22508517120471194| 0.8883889596723192| 0.4649779784735575|\n",
      "|  10| 0.6150792097407688| 0.18247771849608918| 0.5510143369899105|0.28821689001875483| 0.5312132342686203|\n",
      "|  11| 0.8922950281153599|  0.9834522728598483|  0.797933879028158|0.45037558430038616| 0.6199866684586527|\n",
      "|  12|0.14721193869416727|  0.9180260434481775| 0.8000677189394878|  0.765860274463332| 0.1407799463546896|\n",
      "|  13| 0.2780012613089543|   0.892386852739762|0.29101186646297716| 0.7314859284726655|0.05646559733315304|\n",
      "|  14|  0.046909539436227|  0.1921674068440148|0.39028184354311424| 0.1894424880791925|0.30880722673338235|\n",
      "|  15| 0.5351097846636383|  0.4913682256280343|0.12630809776389762|  0.851387839258652| 0.7848497443578581|\n",
      "|  16| 0.9348091495750523| 0.19191369239338762|0.48201770580593506|0.07263990166946444|  0.857665513317544|\n",
      "|  17| 0.7556411499913441|  0.6506737492572015| 0.8499759059099062|0.04772952674473396| 0.9586618625339215|\n",
      "|  18| 0.5126512754662381| 0.13400937158256243|  0.981795980554614|0.48473017084624825|  0.981640332783593|\n",
      "+----+-------------------+--------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile spark_main.py\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# .config(\"spark.jars\", \"spark_jars/hadoop-aws-2.7.3.jar\") \\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.3')\\\n",
    "#.config(\"spark.executor.extraClassPath\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.6')\\\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.6 pyspark-shell'\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "\n",
    "\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"ReadTextFilesFromS3\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4')\\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", \"http://\"+minio_url)\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "jars = sc._jsc.sc().listJars()\n",
    "print(\"recognized additional jars:\",jars)\n",
    "\n",
    "\n",
    "minio_file_path = \"s3a://jupyter-spark-01/random_data.csv\"\n",
    "print(minio_file_path)\n",
    "df = spark.read.csv(minio_file_path,header=False)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22a5ebb-5578-4bdf-b9a9-dae2c6457bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_new_csv_path=\"{}.new.csv\".format(\"random_data.csv\")\n",
    "df.write.format('csv').options(delimiter=',').mode('overwrite').save(minio_new_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
