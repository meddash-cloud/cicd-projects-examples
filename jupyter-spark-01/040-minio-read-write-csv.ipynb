{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91284b97-16a1-48a9-8444-32f1d51f9588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 17.0.8\n",
      "Branch HEAD\n",
      "Compiled by user centos on 2023-06-19T23:01:01Z\n",
      "Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-shell --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9255c3fd-b0e4-4b2a-920d-0ad8a98ae6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#generate 4-columns data\n",
    "data = np.random.rand(1000,5)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"random_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94087c5d-b552-400b-bc8a-21b8d734d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"jupyter-spark-01\"\n",
    "minio_url = \"minio-service.kubeflow.svc.cluster.local:9000\"\n",
    "minio_key = \"minio\"\n",
    "minio_secret = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa36d41-a6b7-474b-b4ba-d0a6e23fc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to minio minio-service.kubeflow.svc.cluster.local:9000\n",
      "try to find bucket jupyter-spark-01\n",
      "found True\n",
      "Bucket 'jupyter-spark-01' already exists\n",
      "upload file to minio...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7f5d5e564d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import os\n",
    "\n",
    "upload_file_name=\"random_data.csv\"\n",
    "upload_file_path=\"./{}\".format(upload_file_name)\n",
    "config = {\n",
    "    \"endpoint\": minio_url,\n",
    "    \"access_key\": \"minio\",\n",
    "    \"secret_key\": \"minio123\",\n",
    "    \"secure\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a client with the MinIO server playground, its access key\n",
    "# and secret key.\n",
    "print (\"connecting to minio {}\".format(minio_url))\n",
    "minio_client = Minio(**config)\n",
    "\n",
    "print(\"try to find bucket {}\".format(BUCKET_NAME))\n",
    "found = minio_client.bucket_exists(BUCKET_NAME)\n",
    "print(\"found\", found)\n",
    "if not found:\n",
    "    minio_client.make_bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(\"Bucket '{}' already exists\".format(BUCKET_NAME))\n",
    "\n",
    "print(\"upload file to minio...\")\n",
    "minio_client.fput_object(BUCKET_NAME, os.path.basename(upload_file_path), upload_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99caecf3-c211-4e06-b157-83c5674d0315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://jupyter-spark-01/random_data.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minio_file_path = 's3a://{}/{}'.format(BUCKET_NAME, upload_file_name)\n",
    "minio_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b898dfa0-199f-4c51-84c7-ef20f9986e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beeline\t\t      pyspark\t\tspark-class.cmd      spark-shell.cmd\n",
      "beeline.cmd\t      pyspark2.cmd\tspark-connect-shell  spark-sql\n",
      "docker-image-tool.sh  pyspark.cmd\tsparkR\t\t     spark-sql2.cmd\n",
      "find-spark-home       run-example\tsparkR2.cmd\t     spark-sql.cmd\n",
      "find-spark-home.cmd   run-example.cmd\tsparkR.cmd\t     spark-submit\n",
      "load-spark-env.cmd    spark-class\tspark-shell\t     spark-submit2.cmd\n",
      "load-spark-env.sh     spark-class2.cmd\tspark-shell2.cmd     spark-submit.cmd\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/spark/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0765c8e3-8c33-4487-bb91-bc87f59c970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf spark_jars\n",
    "mkdir -p spark_jars\n",
    "\n",
    "wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar -P spark_jars\n",
    "wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P spark_jars\n",
    "wget -q https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar -P spark_jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaafe06-0bfa-40e9-853f-9492bb48c58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8062fbee-587c-49e4-8eb5-b6f5d5f4c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognized additional jars: Vector(spark://notebook-sample-v1alpha1-0:40197/jars/com.amazonaws_aws-java-sdk-bundle-1.11.375.jar, spark://notebook-sample-v1alpha1-0:40197/jars/org.apache.hadoop_hadoop-aws-3.2.0.jar)\n",
      "s3a://jupyter-spark-01/random_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=53>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 377, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2255, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o31.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o45.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognized additional jars:\u001b[39m\u001b[38;5;124m\"\u001b[39m,jars)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(minio_file_path)\n\u001b[0;32m---> 37\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminio_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o45.csv"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# .config(\"spark.jars\", \"spark_jars/hadoop-aws-2.7.3.jar\") \\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.3')\\\n",
    "#.config(\"spark.executor.extraClassPath\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars\", f\"{cwd}/spark_jars/hadoop-aws-2.7.3.jar\")\\\n",
    "#.config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "#.config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.6')\\\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.6 pyspark-shell'\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "#.config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"ReadTextFilesFromS3\")\\\n",
    ".master(\"local[*]\")\\\n",
    ".config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.2.0')\\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", minio_url)\\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "jars = sc._jsc.sc().listJars()\n",
    "print(\"recognized additional jars:\",jars)\n",
    "\n",
    "print(minio_file_path)\n",
    "df = spark.read.csv(minio_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a5ebb-5578-4bdf-b9a9-dae2c6457bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635401f-25d9-4563-b39e-3a2226c75703",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://{}\".format(minio_url))\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.access.key\", minio_key)\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.secret.key\", minio_secret )\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "df = spark.read.csv(,header=False)\n",
    "df.write.format('csv').options(delimiter=',').mode('overwrite').save('s3a://{}/{}'.format(BUCKET_NAME, \"{}.overwrite.txt\".fomrat(upload_file_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655421b-a5ff-479b-a483-c9e7fe4f5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/spark/examples/src/main/python/ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a51902-8a0d-4de2-abf2-197ca65173a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a5d63-972f-43c0-ab26-716dd496604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/local/spark/jars|grep -i hadoop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
