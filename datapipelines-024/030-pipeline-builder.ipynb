{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57228fd2-08ae-4415-b5f2-e1aa5cc3e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  --quiet kfp==1.8.22 tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29742442-7e9f-4366-951d-cacd65863272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.3.0\n",
      "Uninstalling tensorflow-2.3.0:\n",
      "  Successfully uninstalled tensorflow-2.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y tensorflow==2.3.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e35d395-a50c-4b08-8d84-557dfa5139b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.4/394.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (2.14.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.11.0)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (3.20.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (0.38.4)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (66.1.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.3.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.7.1)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (1.26.14)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard~=2.4->tensorflow==2.4.1) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.2)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=8b9db75de31dc456bcfbb7a06354b32da6e539b59d2f8012e898b5d027f49fb1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f7/de/e8/f6aec43099b5f725a5227572d0d67a987bc444298b4f14d745\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19553 sha256=ea49615d820438eba193d33f693153e9ffd1606affd90a8ba9ab3010b982d279\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/67/a1/46/13226bbf258069e74ce1598cd4077014852a5e63434be9f1c6\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard-plugin-wit, flatbuffers, tensorboard-data-server, six, numpy, grpcio, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.2.0\n",
      "    Uninstalling termcolor-2.2.0:\n",
      "      Successfully uninstalled termcolor-2.2.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.1\n",
      "    Uninstalling tensorboard-data-server-0.7.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.57.0\n",
      "    Uninstalling grpcio-1.57.0:\n",
      "      Successfully uninstalled grpcio-1.57.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.0\n",
      "    Uninstalling tensorboard-2.14.0:\n",
      "      Successfully uninstalled tensorboard-2.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pydantic 1.10.12 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 grpcio-1.32.0 numpy-1.19.5 six-1.15.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f4f5ef-adb1-4f9e-8af6-b5139bf66073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel to have the new kfp version installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3006e2e8-9572-4a11-8085-cfcb7bb31a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components\n",
    "# from kfp.components import InputPath, InputTextFile, OutputPath, OutputTextFile\n",
    "\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aed017ae-dc31-4856-bc36-682fae8c76b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f62a653-d46e-425f-988e-e91e30d0a57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241m.\u001b[39msave_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "type(tf.keras.saving.save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2481771-e92e-4b4c-a7fa-32d98c6a6fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.22'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c890176a-e531-49f4-8965-ed426489391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-> NamedTuple(\"Output\", [(\"MnistInputDirPath\", str)])\n",
    "# mnist_input_dir_path: OutputPath(str)\n",
    "def put_public_mnist_to_local_minio() :\n",
    "    # import numpy as np\n",
    "    # import pandas as pd\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import tensorflow as tf\n",
    "    # from tensorflow import keras\n",
    "\n",
    "    import tempfile\n",
    "    import os\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "\n",
    "    # from minio.error import S3Error\n",
    "\n",
    "    def get_minio_url():\n",
    "        minio_host, minio_port = os.environ[\"MINIO_SERVICE_SERVICE_HOST\"], os.environ[\"MINIO_SERVICE_SERVICE_PORT_HTTP\"]\n",
    "        minio_url= \"{}:{}\".format(minio_host, minio_port)\n",
    "        return minio_url\n",
    "    minio_url = get_minio_url()\n",
    "    print(\"minio url:\", minio_url)\n",
    "    BUCKET_NAME=\"datapipeline-024\"\n",
    "    config = {\"endpoint\": minio_url,\n",
    "        \"access_key\": \"minio\",\n",
    "        \"secret_key\": \"minio123\",\n",
    "        \"secure\": False}\n",
    "    minio_client = Minio(**config)\n",
    "\n",
    "    print(\"try to find bucket {}\".format(BUCKET_NAME))\n",
    "    found = minio_client.bucket_exists(BUCKET_NAME)\n",
    "    print(\"found\", found)\n",
    "    if not found:\n",
    "        minio_client.make_bucket(BUCKET_NAME)\n",
    "    else:\n",
    "        print(\"Bucket '{}' already exists\".format(BUCKET_NAME))\n",
    "    \n",
    "    import tensorflow.keras as keras\n",
    "    # import keras\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    #np.save generates file path with .npy suffix.\n",
    "    temp_file_paths= [\"/tmp/\"+str(uuid.uuid4()) for i in range(4)]\n",
    "    for file_path, data in zip(temp_file_paths,[x_train, y_train,x_test, y_test]):\n",
    "        print(\"writing to \",file_path+\".npy\")\n",
    "        np.save(file_path,data)\n",
    "\n",
    "    \n",
    "    \n",
    "    minio_client.fput_object(BUCKET_NAME, \"data/original/x_train.npy\", temp_file_paths[0]+\".npy\")\n",
    "    minio_client.fput_object(BUCKET_NAME, \"data/original/y_train.npy\", temp_file_paths[1]+\".npy\")\n",
    "    minio_client.fput_object(BUCKET_NAME, \"data/original/x_test.npy\", temp_file_paths[2]+\".npy\")\n",
    "    minio_client.fput_object(BUCKET_NAME, \"data/original/y_test.npy\", temp_file_paths[3]+\".npy\")\n",
    "    \n",
    "    for f in temp_file_paths:\n",
    "        os.remove(f+\".npy\")\n",
    "\n",
    "    # from collections import namedtuple\n",
    "    # output = namedtuple('Output', ['MnistInputDirPath'])\n",
    "    # return output(mnist_input_dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c64de5-dee0-4f49-a213-b5db2c6e30ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2981d-43e2-419b-b9ea-7ed357a54576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e5efcc-580a-4fd8-8934-f4fbe6bcd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-> NamedTuple(\"Output\", [(\"MnistInputDirPath\", str)])\n",
    "# mnist_input_dir_path: OutputPath(str)\n",
    "def reshape_mnist_and_put_back() :\n",
    "    # import numpy as np\n",
    "    # import pandas as pd\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import tensorflow as tf\n",
    "    # from tensorflow import keras\n",
    "\n",
    "    import tempfile\n",
    "    import os\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "    import glob\n",
    "    \n",
    "    def get_minio_url():\n",
    "        minio_host, minio_port = os.environ[\"MINIO_SERVICE_SERVICE_HOST\"], os.environ[\"MINIO_SERVICE_SERVICE_PORT_HTTP\"]\n",
    "        minio_url= \"{}:{}\".format(minio_host, minio_port)\n",
    "        return minio_url\n",
    "    minio_url = get_minio_url()\n",
    "    print(\"minio url:\", minio_url)\n",
    "    \n",
    "    BUCKET_NAME=\"datapipeline-024\"\n",
    "    config = {\"endpoint\": minio_url,\n",
    "        \"access_key\": \"minio\",\n",
    "        \"secret_key\": \"minio123\",\n",
    "        \"secure\": False}\n",
    "    minio_client = Minio(**config)\n",
    "\n",
    "    \n",
    "    random_prefix=str(uuid.uuid4())\n",
    "    \n",
    "    def download_path(filename):\n",
    "        return \"/tmp/{}_{}.npy\".format(random_prefix,filename)\n",
    "    \n",
    "    for file_name in [\"x_train\", \"x_test\"]:\n",
    "        minio_client.fget_object(BUCKET_NAME,  \"data/original/{}.npy\".format(file_name), download_path(file_name) )\n",
    "        reshaped = np.load(download_path(file_name)).reshape(-1,28,28,1) / 255\n",
    "        np.save(download_path(file_name+\".reshaped\"),reshaped)\n",
    "        minio_client.fput_object(BUCKET_NAME, \"data/original/{}.reshaped.npy\".format(file_name), download_path(file_name+\".reshaped\"))\n",
    "\n",
    "    \n",
    "\n",
    "    # minio_client.fput_object(BUCKET_NAME,  \"data/original/y_train.npy\", download_path(\"y_train\") )\n",
    "    # minio_client.fput_object(BUCKET_NAME,  \"data/original/y_test.npy\", download_path(\"y_test\") )\n",
    "    # y_train = np.load(download_path(\"y_train\")) / 255\n",
    "    # y_test = np.load(download_path(\"y_test\"))\n",
    "    \n",
    "   \n",
    "    for f in glob.glob(\"/tmp/{}_*\".format(random_prefix)):\n",
    "        os.remove(f)\n",
    "\n",
    "    # from collections import namedtuple\n",
    "    # output = namedtuple('Output', ['MnistInputDirPath'])\n",
    "    # return output(mnist_input_dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cd4594ed-3bcb-4a25-8025-42794cc3ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs:int = 1, optimizer: str = \"adam\", model_save_prefix:str=\"models/trained/detect-digits\", version:str = \"1\") :\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    import os\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import shutil\n",
    "# uncomment this function to have a better accuracy\n",
    "#     def build_model():\n",
    "#         model = keras.models.Sequential()\n",
    "#         model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "#         model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "#         model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#         model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "#         model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "#         model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "#         model.add(keras.layers.Flatten())\n",
    "#         model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#         model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "#         model.add(keras.layers.Dense(10, activation='softmax')) #output are 10 classes, numbers from 0-9\n",
    "#         return model\n",
    "\n",
    "    def build_model():\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Conv2D(10, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "        model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(10, (3, 3), activation='relu'))\n",
    "        model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "        model.add(keras.layers.Conv2D(10, (3, 3), activation='relu'))\n",
    "        model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "        model.add(keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "        model.add(keras.layers.Dense(10, activation='softmax')) #output are 10 classes, numbers from 0-9\n",
    "        return model\n",
    "\n",
    "\n",
    "    \n",
    "    def compile_model(_model):\n",
    "        _model.compile(optimizer=optimizer,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    def upload_local_directory_to_minio(minio_client, local_path, bucket_name, minio_path):\n",
    "        # assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    minio_client, local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    \n",
    "    def get_minio_url():\n",
    "        minio_host, minio_port = os.environ[\"MINIO_SERVICE_SERVICE_HOST\"], os.environ[\"MINIO_SERVICE_SERVICE_PORT_HTTP\"]\n",
    "        minio_url= \"{}:{}\".format(minio_host, minio_port)\n",
    "        return minio_url\n",
    "    minio_url = get_minio_url()\n",
    "    \n",
    "    BUCKET_NAME=\"datapipeline-024\"\n",
    "    config = {\"endpoint\": minio_url,\n",
    "        \"access_key\": \"minio\",\n",
    "        \"secret_key\": \"minio123\",\n",
    "        \"secure\": False}\n",
    "    minio_client = Minio(**config)\n",
    "    \n",
    "    random_prefix=str(uuid.uuid4())\n",
    "    def download_path(filename):\n",
    "        return \"/tmp/{}_{}.npy\".format(random_prefix,filename)\n",
    "\n",
    "    \n",
    "    for file_name in [\"y_train\", \"y_test\"]:\n",
    "        minio_client.fget_object(BUCKET_NAME,  \"data/original/{}.npy\".format(file_name), download_path(file_name) )\n",
    "    y_train, y_test = np.load(download_path(\"y_train\")), np.load(download_path(\"y_test\"))\n",
    "    \n",
    "    for file_name in [\"x_train\", \"x_test\"]:\n",
    "        minio_client.fget_object(BUCKET_NAME,  \"data/original/{}.reshaped.npy\".format(file_name), download_path(file_name) )\n",
    "    x_train, x_test = np.load(download_path(\"x_train\")), np.load(download_path(\"x_test\"))\n",
    "    \n",
    "    \n",
    "    model = build_model()\n",
    "    compile_model(model)\n",
    "    \n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    model_saved_path=\"/tmp/saved_model_{}\".format(random_prefix)\n",
    "    # tf.saved_model.save(model, model_saved_path)\n",
    "    # model.save(model, model_saved_path)\n",
    "    tf.keras.models.save_model(model, model_saved_path)\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    # onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    \n",
    "    itisfile=isfile(model_saved_path)\n",
    "    print(\"tf.__version__\", tf.__version__)\n",
    "    print(\"{} : it is file: {}\".format( model_saved_path, itisfile) )\n",
    "    if not itisfile:\n",
    "        print (\"it is directory\")\n",
    "        onlyfiles = [f for f in listdir(model_saved_path) ]\n",
    "        print (\"dir content: \")\n",
    "        print(onlyfiles)\n",
    "    \n",
    "    if itisfile:\n",
    "        print(\"saving as a file\")\n",
    "        minio_client.fput_object(BUCKET_NAME,\"{}/{}/saved_model.pb\".format(model_save_prefix,version), model_saved_path)\n",
    "    else:\n",
    "        print(\"saving as a directory\")\n",
    "        upload_local_directory_to_minio(minio_client, model_saved_path,BUCKET_NAME,\"{}/{}\".format(model_save_prefix,version)) \n",
    "    \n",
    "    # tf.keras.saving.save_model(model, model_saved_path)\n",
    "    \n",
    "#     model.save(model_saved_path)\n",
    "    # minio_client.fput_object(BUCKET_NAME, \"models/trained/model.keras\", model_saved_path)\n",
    "    \n",
    "    # print(\"zipping file:\", model_saved_path+\".zip\")\n",
    "    # print(\"zipping file:\", model_saved_path+\".zip\")\n",
    "\n",
    "    # shutil.make_archive(model_saved_path+\".zip\", 'zip', model_saved_path)\n",
    "    # minio_client.fput_object(BUCKET_NAME, \"models/trained/model.keras\", model_saved_path)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e96301-ac6b-4c4d-a9db-7529dcce8212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f6f84adc-367a-4f89-83a5-b90e74142414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "def test_model_and_save_metrics(model_save_prefix:str=\"models/trained/detect-digits\", version:str=\"1\") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]) :\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    import os\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import uuid\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import shutil\n",
    "    \n",
    "    # generate confusion matrix csv\n",
    "    def gen_cm_csv(y_test=None,test_predictions=None):\n",
    "        confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "        confusion_matrix = confusion_matrix.numpy()\n",
    "        vocab = list(np.unique(y_test))\n",
    "        data = []\n",
    "        for target_index, target_row in enumerate(confusion_matrix):\n",
    "            for predicted_index, count in enumerate(target_row):\n",
    "                data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "        df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "        cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "        return cm_csv\n",
    "    \n",
    "    \n",
    "    def get_minio_url():\n",
    "        minio_host, minio_port = os.environ[\"MINIO_SERVICE_SERVICE_HOST\"], os.environ[\"MINIO_SERVICE_SERVICE_PORT_HTTP\"]\n",
    "        minio_url= \"{}:{}\".format(minio_host, minio_port)\n",
    "        return minio_url\n",
    "    minio_url = get_minio_url()\n",
    "    \n",
    "    BUCKET_NAME=\"datapipeline-024\"\n",
    "    config = {\"endpoint\": minio_url,\n",
    "        \"access_key\": \"minio\",\n",
    "        \"secret_key\": \"minio123\",\n",
    "        \"secure\": False}\n",
    "    minio_client = Minio(**config)\n",
    "    \n",
    "    random_prefix=str(uuid.uuid4())\n",
    "    def download_path(filename):\n",
    "        return \"/tmp/{}_{}.npy\".format(random_prefix,filename)\n",
    "\n",
    "\n",
    "    model_remote_path=\"{}/{}\".format(model_save_prefix, version)\n",
    "    model_saved_path=\"/tmp/{}/{}\".format(model_save_prefix, version)\n",
    "    # minio_client.fget_object(BUCKET_NAME,model_remote_path,model_saved_path)\n",
    "    \n",
    "    for bucket in minio_client.list_buckets():\n",
    "        if bucket.name!=BUCKET_NAME:\n",
    "            continue\n",
    "        for item in minio_client.list_objects(bucket.name,model_remote_path,recursive=True):\n",
    "            print(\"remote name:\",item.object_name)\n",
    "            print(\"local name:\", \"/tmp/\"+item.object_name)\n",
    "            minio_client.fget_object(bucket.name,item.object_name, \"/tmp/\"+item.object_name)\n",
    "    \n",
    "    \n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    \n",
    "\n",
    "    itisfile=isfile(model_saved_path)\n",
    "    print(\"{} : it is file: {}\".format( model_saved_path, itisfile) )\n",
    "    if not itisfile:\n",
    "        print (\"it is directory\")\n",
    "        onlyfiles = [f for f in listdir(model_saved_path) ]\n",
    "        print (\"dir content: \")\n",
    "        print(onlyfiles)\n",
    "    \n",
    "    # model = tf.keras.models.load_model(\"/tmp/{}/{}\".format(model_save_prefix, version))\n",
    "    # model = tf.keras.saving.load_model(\"/tmp/{}/{}\".format(model_save_prefix, version))\n",
    "    print (\"loading model in path:\", model_saved_path)\n",
    "    # model = tf.saved_model.load(model_saved_path)\n",
    "    model = tf.keras.models.load_model(model_saved_path)\n",
    "    \n",
    "    for file_name in [\"x_test\", \"y_test\"]:\n",
    "        minio_client.fget_object(BUCKET_NAME,  \"data/original/{}.npy\".format(file_name), download_path(file_name) )\n",
    "    x_test, y_test = np.load(download_path(\"x_test\")), np.load(download_path(\"y_test\"))\n",
    "    \n",
    "    \n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    \n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) \n",
    "    \n",
    "    cm_csv = gen_cm_csv(y_test=y_test,test_predictions=test_predictions)\n",
    "    \n",
    "    #show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    output_confussion_matrix = {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": list(np.arange(10)) #0..9 labels\n",
    "            }\n",
    "    output_model_summary = {\n",
    "                'type': 'markdown',\n",
    "                'storage': 'inline',\n",
    "                'source': f'''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{metric_model_summary}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {model_accuracy}\n",
    "**Loss**: {model_loss}\n",
    "\n",
    "'''\n",
    "            }\n",
    "    \n",
    "    metadata = {\"outputs\": [output_confussion_matrix, output_model_summary]}\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    \n",
    "    class NpJsonEncoder(json.JSONEncoder):\n",
    "        \"\"\"Serializes numpy objects as json.\"\"\"\n",
    "\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.bool_):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                if np.isnan(obj):\n",
    "                    return None  # Serialized as JSON null.\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return super().default(obj)\n",
    "        \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('Output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata, cls=NpJsonEncoder),json.dumps(metrics, cls=NpJsonEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5d39679c-1526-497e-88bc-206abe95a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components\n",
    "\n",
    "step_put_public_mnist_to_local_minio = components.create_component_from_func(put_public_mnist_to_local_minio,base_image=\"kubeflownotebookswg/jupyter-tensorflow-full\")\n",
    "step_reshape_mnist_and_put_back = components.create_component_from_func(reshape_mnist_and_put_back,base_image=\"kubeflownotebookswg/jupyter-tensorflow-full\")\n",
    "step_train_model = components.create_component_from_func(train_model,base_image=\"kubeflownotebookswg/jupyter-tensorflow-full\")\n",
    "step_test_model_and_save_metrics = components.create_component_from_func(test_model_and_save_metrics,base_image=\"kubeflownotebookswg/jupyter-tensorflow-full\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='kfp-pipeline-digits-clsfier',\n",
    "    description='Classify digits'\n",
    ")\n",
    "def kfp_pipeline_start(epochs,optimizer,model_save_prefix, version):\n",
    "    step1 = step_put_public_mnist_to_local_minio()\n",
    "    step2 = step_reshape_mnist_and_put_back()\n",
    "    step3 = step_train_model(epochs,optimizer,model_save_prefix, version)\n",
    "    step4 = step_test_model_and_save_metrics(model_save_prefix, version)\n",
    "\n",
    "    step2.after(step1)\n",
    "    step3.after(step2)\n",
    "    step4.after(step3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4a93fd91-ba15-4d39-a56f-297bed18b633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.152.183.66:8888/#/experiments/details/e40df800-a1c6-48a7-9302-aea840cc7f58\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.152.183.66:8888/#/runs/details/b609cbea-7d86-49ef-af49-44a09c6de283\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "host, port = os.environ[\"ML_PIPELINE_PORT_8888_TCP_ADDR\"],os.environ[\"ML_PIPELINE_SERVICE_PORT_HTTP\"]\n",
    "kfp_endpoint=\"http://{}:{}\".format(host,port)\n",
    "\n",
    "kfp_client = kfp.Client(host=kfp_endpoint)\n",
    "pipeline_arguments = {\n",
    "    \"epochs\": 1,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"model_save_prefix\": \"models/trained/detect-digits\",\n",
    "    \"version\": \"13\"\n",
    "}\n",
    "pipeline_run = kfp_client.create_run_from_pipeline_func(kfp_pipeline_start, arguments=pipeline_arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "387757e6-c3c7-4c68-90a0-da8d924c0f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.run_id: b609cbea-7d86-49ef-af49-44a09c6de283\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"http://kf-ml-pipeline-ui.v1005r7c-bfef-4f0b-8028-99d5fdcf47de.hosted.meddash.cloud/#/runs/details/b609cbea-7d86-49ef-af49-44a09c6de283\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f5a2e8bcb50>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import IFrame\n",
    "cluster_id= os.environ[\"MEDDASH_CLUSTER_ID\"]\n",
    "run_id=pipeline_run.run_id\n",
    "print(\"pipeline.run_id:\", run_id)\n",
    "pipeline_vis_endpoint=\"http://kf-ml-pipeline-ui.{}.hosted.meddash.cloud/#/runs/details/{}\".format(cluster_id,run_id)\n",
    "IFrame(pipeline_vis_endpoint, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5893318-0443-46e5-9faa-0c8baac7b948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ece2f1-213d-4e15-b0a5-92fd5f6650c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56809a1-510a-4bb6-b92f-aa1455a86402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ec51c-7a0a-4366-b41d-3d7effd4029f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "17ac2773-ccb6-495e-b5e7-2d03620b5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket name: datapipeline-024\n",
      "models/trained/detect-digits/5/saved_model.pb\n",
      "models/trained/detect-digits/5/variables/variables.data-00000-of-00001\n",
      "models/trained/detect-digits/5/variables/variables.index\n",
      "bucket name: mlpipeline\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "from minio import Minio\n",
    "import numpy as np\n",
    "import uuid\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def get_minio_url():\n",
    "    minio_host, minio_port = os.environ[\"MINIO_SERVICE_SERVICE_HOST\"], os.environ[\"MINIO_SERVICE_SERVICE_PORT_HTTP\"]\n",
    "    minio_url= \"{}:{}\".format(minio_host, minio_port)\n",
    "    return minio_url\n",
    "minio_url = get_minio_url()\n",
    "\n",
    "BUCKET_NAME=\"datapipeline-024\"\n",
    "config = {\"endpoint\": minio_url,\n",
    "    \"access_key\": \"minio\",\n",
    "    \"secret_key\": \"minio123\",\n",
    "    \"secure\": False}\n",
    "minio_client = Minio(**config)\n",
    "\n",
    "random_prefix=str(uuid.uuid4())\n",
    "def download_path(filename):\n",
    "    return \"/tmp/{}_{}\".format(random_prefix,filename)\n",
    "\n",
    "    \n",
    "\n",
    "version=\"5\"\n",
    "\n",
    "for bucket in minio_client.list_buckets():\n",
    "    if bucket.name!=BUCKET_NAME:\n",
    "        continue\n",
    "    for item in minio_client.list_objects(bucket.name,\"models/trained/detect-digits/5\",recursive=True):\n",
    "        print(item.object_name)\n",
    "        minio_client.fget_object(bucket.name,item.object_name,\"/tmp/\"+item.object_name)\n",
    "\n",
    "\n",
    "# minio_client.fget_object(BUCKET_NAME, \"models/trained/detect-digits/5\", download_path(\"model.keras\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42e47c93-971f-4f94-a9f8-1027e1c6eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/models/trained/detect-digits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dc3eb26e-9c89-404b-883b-e3bc26bb6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
